{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb1b841-3835-49a9-ad30-cd97bf0a50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4d6c79-d725-4fa0-874e-2b70d4df859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target         ids                          date      flag  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\"C:/Kaggle/training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", names=column_names)\n",
    "print(df.head())\n",
    "print(df[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e96f8fd-911e-4967-be23-2ad8a8835b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    " text = re.sub(r\"http\\S+\", \"\", text) # URL 除去\n",
    " text = re.sub(r\"@\\w+\", \"\", text) # メンション除去\n",
    " text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text) # 記号除去\n",
    " return text.lower() # 小文字化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a521d3-bf70-4bbf-bcc6-a44116536e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       awww thats a bummer  you shoulda got david ...\n",
      "1    is upset that he cant update his facebook by t...\n",
      "2     i dived many times for the ball managed to sa...\n",
      "3      my whole body feels itchy and like its on fire \n",
      "4     no its not behaving at all im mad why am i he...\n",
      "Name: clean_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "print(df[\"clean_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78aee8f-93ff-4e1f-ba23-41e2750e6a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# ベクトル化（上位 5000 語に限定、英語ストップワード除外）\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c179eb3-6910-4b48-9d2e-fd0f95ac9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683a4729-b4cd-4a82-823d-719b0baad740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: just | got | home | lol | woke | new | twitter | time | ive | finished\n",
      "Topic 2: good | morning | night | luck | thats | hope | time | sounds | feeling | feel\n",
      "Topic 3: im | sorry | gonna | tired | sad | sure | lol | bored | sick | right\n",
      "Topic 4: thanks | follow | following | lol | great | followfriday | hey | haha | ill | ff\n",
      "Topic 5: work | tomorrow | today | home | ready | time | getting | doesnt | hours | didnt\n",
      "Topic 6: love | song | new | thank | haha | lt | lol | oh | watching | guys\n",
      "Topic 7: dont | know | want | really | think | lol | feel | wanna | let | need\n",
      "Topic 8: day | today | happy | great | mothers | hope | nice | tomorrow | school | long\n",
      "Topic 9: miss | really | gonna | home | ill | come | friends | guys | baby | days\n",
      "Topic 10: going | like | today | time | feel | bed | night | sleep | tomorrow | wish\n"
     ]
    }
   ],
   "source": [
    "# NMF モデルの定義と学習\n",
    "nmf = NMF(n_components=n_topics, random_state=0)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(H):\n",
    " top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
    " print(f\"Topic {topic_idx + 1}: {' | '.join(top_words)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
